"""
LLM API ä»£ç†æœåŠ¡
å°† API_KEY ç­‰æ•æ„Ÿä¿¡æ¯å°è£…åœ¨æœåŠ¡ç«¯ï¼Œå‰ç«¯é€šè¿‡æ­¤æ¥å£ä¸ LLM é€šä¿¡
"""

import os
from typing import List, Optional
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
import httpx
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

app = FastAPI(title="Know Yourself LLM Proxy")

# CORS é…ç½® - å…è®¸å‰ç«¯è·¨åŸŸè®¿é—®
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "http://localhost:5173",
        "http://localhost:5174",
        "http://localhost:5175",
        "http://localhost:5176",
        "https://yippee701.github.io",  # GitHub Pages
    ],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# API é…ç½®ï¼ˆä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
API_CONFIG = {
    "base_url": os.getenv("API_BASE_URL", "https://api.openai.com/v1"),
    "api_key": os.getenv("API_KEY", ""),
    "model": os.getenv("MODEL", "gpt-4o-mini"),
    "max_tokens": int(os.getenv("MAX_TOKENS", "8192")),
}

# ç³»ç»Ÿæç¤ºè¯
SYSTEM_PROMPT = """#Roleï¼šæ·±åº¦å¤©èµ‹æŒ–æ˜æœº
#è§’è‰²
ä½ æ˜¯ä¸€ä½ç»“åˆäº†ç›–æ´›æ™®ä¼˜åŠ¿ç†è®ºã€å¿ƒæµç†è®ºä¸è£æ ¼å¿ƒç†å­¦çš„èµ„æ·±ç”Ÿæ¶¯å’¨è¯¢å¸ˆã€‚ä½ åšä¿¡å¤©èµ‹ä¸æ˜¯æŸç§å…·ä½“æŠ€èƒ½ï¼Œè€Œæ˜¯å¯è¿ç§»çš„åº•å±‚èƒ½åŠ›ã€‚
#ç›®æ ‡
é€šè¿‡å¤šä¸ªæ·±åº¦å¤šè½®å¯¹è¯ï¼Œå¸®åŠ©ç”¨æˆ·æ‰“ç ´ç„¦è™‘ï¼Œå¸®ä»–ä»¬æ‰¾åˆ°ä»–ä»¬è¢«å½±è—èµ·æ¥çš„å¤©èµ‹ï¼Œå¹¶ç”Ÿæˆä¸€ä»½æåº¦è¯¦ç»†ã€ä¸“ä¸šæœ‰å…±æƒ…åŠ›çš„ã€Šå¤©èµ‹è¯´æ˜ä¹¦ã€‹ã€‚
#æ ¸å¿ƒç†å¿µ
1.åå®¿å‘½è®ºã€‚2.èƒ½é‡å®¡è®¡ï¼šçœŸæ­£çš„å¤©èµ‹æ˜¯è®©ä½ å›è¡€çš„äº‹ï¼Œè€Œä¸æ˜¯ä½ å•çº¯æ“…é•¿ä½†åšå®Œå¾ˆç´¯çš„äº‹ã€‚3.é˜´å½±å³å®è—ï¼šç”¨æˆ·çš„ç¼ºç‚¹ã€æ€ªç™–ã€ç”šè‡³å¯¹ä»–äººçš„å«‰å¦’ï¼Œå¾€å¾€æ˜¯å¤©èµ‹è¢«å‹æŠ‘çš„èƒŒé¢ã€‚
#ä¸¥æ ¼éµå®ˆ
1.ç¦æ­¢ä¸€æ¬¡æ€§æé—®ï¼šå¿…é¡»é‡‡ç”¨"ä½ é—®->ç”¨æˆ·ç­”->ä½ ç®€çŸ­åé¦ˆ->å†é—®ä¸‹ä¸€é¢˜"çš„æ¨¡å¼ã€‚æ¯è½®å¯¹è¯åªèšç„¦ä¸€ä¸ªé—®é¢˜ã€‚
2.è‹æ ¼æ‹‰åº•å¼å¼•å¯¼ï¼šä¸è¦æ€¥ç€ä¸‹ç»“è®ºï¼Œå¤šé—®"ä¸ºä»€ä¹ˆ"ã€"å½“æ—¶ä»€ä¹ˆæ„Ÿè§‰"ã€"å…·ä½“ä¾‹å­".
3.æ¸©æš–è€ŒçŠ€åˆ©ï¼šä¿æŒå…±æƒ…ï¼Œä½†åœ¨æ•æ‰ç”¨æˆ·é€»è¾‘æ¼æ´æˆ–æ½œæ„è¯†ä¿¡å·æ—¶è¦æ•é”ã€‚
#æé—®é—®é¢˜
æé—®1ï¼šè¯·å¼•å¯¼ç”¨æˆ·å›å¿†16å²ä¹‹å‰ï¼ˆæœªè¢«ç¤¾ä¼šå®Œå…¨è§„è®­å‰ï¼‰ï¼Œæœ‰å“ªäº›äº‹æƒ…æ˜¯æ²¡äººé€¼ä¹Ÿä¼šåºŸå¯å¿˜é£Ÿå»åšçš„ï¼Ÿæˆ–è€…æœ‰å“ªäº›ä»å°åˆ°å¤§è¢«æ‰¹è¯„çš„"é¡½å›ºç¼ºç‚¹"ï¼ˆå¦‚çˆ±æ’å˜´ã€å¤ªæ•æ„Ÿã€çˆ±å‘å‘†ï¼‰ï¼Ÿ
æé—®2ï¼šæˆå¹´åçš„å·¥ä½œ/ç”Ÿæ´»ä¸­ï¼Œå“ªä»¶äº‹è®©ä½ è§‰å¾—"è¿™è¿˜éœ€è¦å­¦å—ï¼Ÿè¿™ä¸æ˜¯æ˜¾è€Œæ˜“è§çš„å—ï¼Ÿ"ä½†å‘¨å›´äººå´è§‰å¾—å¾ˆéš¾ï¼Ÿï¼ˆå¯»æ‰¾æ— æ„è¯†èƒœä»»åŒºï¼‰ã€‚
æé—®3ï¼šå“ªä»¶äº‹åšå®Œåè™½ç„¶èº«ä½“ç´¯ï¼Œä½†ç²¾ç¥æåº¦äº¢å¥‹ï¼Ÿ
æé—®4ï¼šè¿™å¯èƒ½æœ‰ç‚¹å†’çŠ¯ï¼Œä½†å¾ˆå…³é”®ï¼Œä½ æ›¾ç»å¯¹è°ï¼ˆæˆ–å“ªç§ç”Ÿæ´»çŠ¶æ€ï¼‰äº§ç”Ÿè¿‡å¼ºçƒˆçš„å«‰å¦’æˆ–é…¸æºœæºœçš„æ„Ÿè§‰ï¼Ÿï¼ˆå«‰å¦’é€šå¸¸æ˜¯"è¢«å‹æŠ‘çš„å¤©èµ‹"åœ¨å‘å‡ºä¿¡å·ï¼Œè¯·è¯šå®é¢å¯¹ï¼‰.
è¿™å››ä¸ªé—®é¢˜å¿…é¡»é—®åˆ°ï¼Œä½†æ˜¯ä¸ä¸€å®šæ˜¯çº¿æ€§çš„ï¼Œè¿‡ç¨‹ä¸­ä¹Ÿå¯ä»¥æ ¹æ®ä½ å¯¹ç”¨æˆ·çš„å¥½å¥‡å’ŒæŒ–æ˜ï¼Œæ¥æå‡ºå…¨æ–°çš„é—®é¢˜ï¼Œåªè¦å¯¹å‘æ˜ç”¨æˆ·çš„å¤©èµ‹æœ‰å¸®åŠ©ã€‚æœ€å¤šä¸è¶…è¿‡10ä¸ªé—®é¢˜.
#è¾“å‡º
ç»¼åˆæ‰€æœ‰é—®é¢˜çš„ä¿¡æ¯ï¼Œè¾“å‡ºä¸‡å­—å·¦å³çš„ã€Šä¸ªäººå¤©èµ‹ä½¿ç”¨è¯´æ˜ä¹¦ã€‹ã€‚è¿™ç¯‡æŠ¥å‘Šä¸è®¾å®šç»“æ„ï¼Œç”±ä½ æ ¹æ®ç”¨æˆ·çš„ç­”æ¡ˆï¼Œè‡ªç”±å‘æŒ¥ã€‚ä½†å¿…é¡»ä¸€ä¸‡å­—ä»¥ä¸Šï¼Œéœ€è¦è¾¾åˆ°ä»–çš„å†…å¿ƒï¼Œè®©ä»–çœŸçš„è§‰å¾—æœ‰ç”¨ï¼Œå¸®åŠ©ä»–æ‰¾åˆ°çœŸæ­£çš„åº•å±‚å¤©èµ‹ï¼Œä¸ºä»–æœªæ¥çš„äººç”Ÿè·¯å’Œä»äº‹èŒä¸šç»™ä¸è¯¦ç»†çš„å»ºè®®ã€‚
#å¼€å§‹
è¯·ä»¥æ¸©æš–ã€ä¸“ä¸šã€å…±æƒ…çš„è¯­è°ƒå¼€åœºï¼Œåƒç”¨æˆ·è¯¦ç»†è§£é‡Šæ¥ä¸‹æ¥çš„æµç¨‹å’Œå ç”¨çš„æ—¶é—´ï¼Œä»¥åŠå¸Œæœ›è¾¾æˆçš„ç›®æ ‡ã€‚å‘ç”¨æˆ·é—®å¥½ï¼Œç”¨é€šä¿—è¯­è¨€ç®€è¿°å¤©èµ‹æŒ–æ˜æœºçš„ä½œç”¨ï¼Œå‘Šè¯‰ç”¨æˆ·ï¼š"å¤©èµ‹æ°¸è¿œä¸ä¼šè¿‡æœŸï¼Œæˆ‘ä»¬åªæ˜¯è¦æ‰¾åˆ°ä½ çš„åº•å±‚å¤©èµ‹ã€‚"ç„¶ååœ¨å†å¼€å§‹è¿›å…¥æé—®æµç¨‹ã€‚"""


class Message(BaseModel):
    role: str  # 'user' | 'assistant'
    content: str


class ChatRequest(BaseModel):
    messages: List[Message]
    stream: Optional[bool] = True

@app.get("/")
async def hello_world():
    return {"message": "Hello, SAE!"}
    
@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥æ¥å£"""
    return {"status": "ok", "model": API_CONFIG["model"]}


@app.post("/chat")
async def chat(request: ChatRequest):
    """
    èŠå¤©æ¥å£ - æ”¯æŒæµå¼å’Œéæµå¼è¾“å‡º
    """
    if not API_CONFIG["api_key"]:
        raise HTTPException(status_code=500, detail="API_KEY æœªé…ç½®")

    # æ„å»ºè¯·æ±‚æ¶ˆæ¯
    messages = [{"role": "system", "content": SYSTEM_PROMPT}]
    messages.extend([{"role": msg.role, "content": msg.content} for msg in request.messages])

    payload = {
        "model": API_CONFIG["model"],
        "messages": messages,
        "stream": request.stream,
        "temperature": 0.7,
        "max_tokens": API_CONFIG["max_tokens"],
    }

    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {API_CONFIG['api_key']}",
    }

    if request.stream:
        # æµå¼è¾“å‡º
        return StreamingResponse(
            stream_chat(payload, headers),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
            },
        )
    else:
        # éæµå¼è¾“å‡º
        async with httpx.AsyncClient(timeout=120.0) as client:
            response = await client.post(
                f"{API_CONFIG['base_url']}/chat/completions",
                json=payload,
                headers=headers,
            )
            
            if response.status_code != 200:
                raise HTTPException(
                    status_code=response.status_code,
                    detail=f"LLM API é”™è¯¯: {response.text}",
                )
            
            data = response.json()
            content = data.get("choices", [{}])[0].get("message", {}).get("content", "")
            return {"content": content}


async def stream_chat(payload: dict, headers: dict):
    """
    æµå¼èŠå¤©ç”Ÿæˆå™¨
    """
    async with httpx.AsyncClient(timeout=120.0) as client:
        async with client.stream(
            "POST",
            f"{API_CONFIG['base_url']}/chat/completions",
            json=payload,
            headers=headers,
        ) as response:
            if response.status_code != 200:
                error_text = await response.aread()
                yield f"data: {{'error': '{error_text.decode()}'}}\n\n"
                return

            async for line in response.aiter_lines():
                if line.strip():
                    yield f"{line}\n\n"
            
            yield "data: [DONE]\n\n"


if __name__ == "__main__":
    import uvicorn
    
    port = int(os.getenv("PORT", "80"))
    print(f"ğŸš€ å¯åŠ¨æœåŠ¡: http://localhost:{port}")
    print(f"ğŸ“– API æ–‡æ¡£: http://localhost:{port}/docs")
    
    uvicorn.run(app, host="0.0.0.0", port=port)

